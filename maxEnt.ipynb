{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537573a0",
   "metadata": {},
   "source": [
    "## Applying MaxEnt to explore the inner working of GPT2 \n",
    "\n",
    "Let $\\vec e_1, \\vec e_2, \\dots, \\vec e_{n+1}$ be the tokens in the input prompt. We'd like to determine:\n",
    "$$\n",
    "p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)\n",
    "$$\n",
    "which is essentially what the transformer computes after $12$ repetitions of the ATTN+FFNN mechanism. In principle, since the action of the transformer (_the dynamical rule_) is known, one could potentially figure out exactly the resulting pdf in the vocabulary space. This is however analytically infeasible, since the transformer is made of several non linear layers (FFNN) and the dynamics is allegedely extremely complicated. Thus, analoguosly to what we do in stat mech, we hope our system is ergodic and try to build a statistical description on top of the whole embedding space.\n",
    "\n",
    " In particular, we resort to the __MaxEnt principle__ to compute (analytically or not) the final conditioned pdf. Note that we do have the \"correct\" pdf $p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)$. We just want to see whether, upon applying MaxEnt with the right constraints, we can obtain something similar to the distribution computed by the transformer\n",
    "\n",
    " ### Bolztmann-like approach\n",
    " The entropy functional on reads:\n",
    "\n",
    "$$\n",
    "S[p] = -k \\int d^D x \\> p(\\vec x) \\ln p(\\vec x) \n",
    "$$\n",
    "the first constraint is, obviously, the normalization $ \\int d^D x \\> p(\\vec x) = 1 $\n",
    "\n",
    "In this first paragraph, we apply the following constraint:\n",
    "$$\n",
    "\\langle \\vec x \\rangle = \\vec{e}_{n+1}\n",
    "$$\n",
    "because we do have the \"right\" next token (again, we don't want to perform prediction here with the maxEnt. We just wish to see if the resulting pdf has some similarities with the one suggested by the maximization of $S$).\n",
    "\n",
    "This leads to an analytic form for $p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)$:\n",
    "$$\n",
    "p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n) = p(\\vec x) = \\frac{1}{Z} \\exp(\\vec \\mu \\cdot \\vec x)\n",
    "$$\n",
    "with $Z =\\prod_{i=1}^{D} \\frac{2 sinh(\\mu_i)}{\\mu_i}$ and the vector $\\mu$ (the lagrange multiplier) is such that:\n",
    "$$\n",
    "\\vec e_{n+1} = -\\frac{1}{\\vec\\mu} + \\frac{1}{\\tanh(\\vec\\mu)} \\Longleftrightarrow \\vec\\mu = f(\\vec e_{n+1})\n",
    "$$\n",
    "\n",
    "Practically speaking, we have built a pdf Boltzmann-like where the expected value has to be the correct next token and the mode (the vector in the embedding space that maximizes the probability) is precisely $\\vec \\mu$. Let's see what $\\vec \\mu$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfe862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT AND CONFIGURATION\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True, output_attentions=True)\n",
    "embeddings = model.wte.weight.data\n",
    "\n",
    "def get_tokens_prob(x, k=5):\n",
    "    # Compute the similarity between the token and all the tokens in the vocabulary, then applies softmax\n",
    "    prob = torch.softmax(torch.matmul(x,embeddings.T), dim=-1)\n",
    "    # Get the top k tokens\n",
    "    top = torch.topk(prob, k=k)\n",
    "    idxs = top.indices\n",
    "    tokens = [tokenizer.decode(idx) for idx in idxs]\n",
    "    return top.values, idxs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc2238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token prediction:\n",
      " -> logp = 0.28: [' Galile'], idx = 32422\n",
      " -> logp = 2.29: ['.'], idx = 13\n",
      " -> logp = 3.12: [' and'], idx = 290\n",
      " -> logp = 3.55: [','], idx = 11\n",
      " -> logp = 5.17: ['.\"'], idx = 526\n",
      " -> logp = 5.62: [' ('], idx = 357\n",
      " -> logp = 5.87: [' Galileo'], idx = 45860\n",
      " -> logp = 5.99: [' in'], idx = 287\n",
      " -> logp = 6.42: [' I'], idx = 314\n",
      " -> logp = 6.75: [' de'], idx = 390\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE STEP\n",
    "\n",
    "# The prompt\n",
    "prompt = \"I'm an Italian physicist who lived in Padua. I am known for my work in the field of optics and astronomy. \" \\\n",
    "        \"I was born in 1564 and died in 1642. My name is Galileo\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states\n",
    "layers = hidden_states[1:]\n",
    " \n",
    "# Next output token prediction\n",
    "nextToken = (layers[-1][0])[-1]\n",
    "# Given this representation of the next token, we project it into the vocabulary space and build a pdf on top of it using softmax\n",
    "lengthQueue = 10\n",
    "prob, idxs, tokens = get_tokens_prob(nextToken, lengthQueue)\n",
    "print(\"Next token prediction:\")\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> logp = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3cb2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully computed mu vector.\n",
      "It represents the direction of maximum probability; projecting mu over the vocabolary we get: \n",
      "\n",
      " -> logp = -0.00: [' Galile'], idx = 32422\n",
      " -> logp = 38.92: [' Canaver'], idx = 46858\n",
      " -> logp = 40.84: ['Palest'], idx = 32570\n",
      "\n",
      "As expected, it points practically with certainty to the right expected token\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "#This is the function we want to solve\n",
    "def f(x,k):\n",
    "    return 1 / np.tanh(x) - 1 / x - k\n",
    "\n",
    "# We need to constraint the mean value of the distribution to be equal to the expected value of the token\n",
    "tokenExpected = embeddings[32422]      # extracted from above\n",
    "\n",
    "mu = torch.tensor(np.zeros(embeddings.shape[1]), dtype=torch.float32)\n",
    "for i,d in enumerate(tokenExpected):\n",
    "    xi = d.item()\n",
    "    mu_i = fsolve(f, 1, args=(xi))\n",
    "    mu[i] = (mu_i.item())\n",
    "\n",
    "print(f\"Succesfully computed mu vector.\")\n",
    "print(\"It represents the direction of maximum probability; projecting mu over the vocabolary we get: \\n\")\n",
    "# mu should be the direction of maximum probability (mode). At what does it point to?\n",
    "prob, idxs, tokens = get_tokens_prob(mu, 3)\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> logp = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")\n",
    "\n",
    "print(\"\\nAs expected, it points practically with certainty to the right expected token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed2a02",
   "metadata": {},
   "source": [
    "As expected, $\\vec \\mu$ (the mode, determined with MaxEnt) is essentially identical to the mean value (that we imposed as the right token)\n",
    "\n",
    "\n",
    "\n",
    "DA AGGIUNGERE: LOG-LIKELI ESPONENZIALE COL PRODOTTO SCALARE, DA AGGIUNGERE? GIUSTIFICAZIONE A POSTERIORI\n",
    "\n",
    "\n",
    "### Using the attention matrix\n",
    "\n",
    "The previous approach was clearly extremely simple and trivial, lacking precious information that we have on the resulting pdf (in fact, we didn't use the previous tokens $\\{e_i\\}$). Let us think of a new constraint. Since the dot product in the embedding space (upon normalization) is somehow related to the similarities between tokens, we may wish to constrain:\n",
    "$$ \n",
    "\\langle \\vec x \\cdot \\vec e_i \\rangle = something, \\>\\> \\forall \\vec e_i\n",
    "$$\n",
    "Since the transfomer provides us with attention matrices, why don't use those quantities to constrain our entropy. In particular, if $\\hat A$ is the $n+1 \\times n+1$ attention matrix at the end of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a380e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the last token (Einstein), GPT2 would have predicted:\n",
      " -> logp = 0.58: [' Einstein'], idx = 24572\n",
      " -> logp = 4.39: [' B'], idx = 347\n",
      " -> logp = 4.67: [' von'], idx = 18042\n",
      " -> logp = 4.79: [' Hof'], idx = 37745\n",
      " -> logp = 4.83: ['.'], idx = 13\n",
      " -> logp = 4.86: [' H'], idx = 367\n",
      " -> logp = 4.94: [' Z'], idx = 1168\n",
      " -> logp = 5.00: [' J'], idx = 449\n",
      " -> logp = 5.07: [' Sch'], idx = 3059\n",
      " -> logp = 5.08: [' A'], idx = 317\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE STEP. HERE, WE LET GPT2 PREDICT THE NEXT TOKEN. WE WILL USE PART OF THE RESULT TO PERFORM OUR STATISTICAL MAXENT ANALYSIS\n",
    "\n",
    "# The prompt\n",
    "prompt = \"I'm a German-born theoretical physicist who revolutionized modern physics with my theory of relativity. \" \\\n",
    "\"I was awarded the Nobel Prize in Physics in 1921 for my explanation of the photoelectric effect.\" \\\n",
    "\" I was born in 1879 and died in 1955. My name is Albert Einstein\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(' '.join(prompt.split()[:-1]), return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states\n",
    "layers = hidden_states[1:]\n",
    "\n",
    "# attentions is a tuple of tensors. Each tensor is 12 attention maps of 16x16 matrices.\n",
    "attentions = outputs.attentions\n",
    "\n",
    "n = len(inputs[\"input_ids\"][0])    # total number of tokens in the input\n",
    "\n",
    "nextToken = (layers[-1][0])[-1]\n",
    "prob, idxs, tokens = get_tokens_prob(nextToken, lengthQueue)\n",
    "print(\"Masking the last token (Einstein), GPT2 would have predicted:\")\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> logp = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3ee5978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastStepAttentionMatrices = attentions[0][0]\n",
    "vectorA = torch.tensor(np.zeros(n-1))              # Not the last token (itself w/ itself)\n",
    "#iterate on the heads\n",
    "for head in range(12):\n",
    "    attentionHead = lastStepAttentionMatrices[head]\n",
    "    column = attentionHead[-1,:]\n",
    "    column = column[:-1]\n",
    "    vectorA = (vectorA + column)\n",
    "\n",
    "T = torch.nn.functional.one_hot(inputs.input_ids[0], num_classes=embeddings.shape[0]).float()\n",
    "E = torch.matmul(T,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40183197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a German-born theoretical physicist who revolutionized modern physics with my theory of relativity. I was awarded the Nobel Prize in Physics in 1921 for my explanation of the photoelectric effect. I was born in 1879 and died in 1955. My name is Albert\n",
      " -> 0.81: ' mathemat'\n",
      " -> 0.95: ' physicist'\n",
      " -> 1.93: ' physicists'\n",
      " -> 4.26: ' Physics'\n",
      " -> 5.79: ' Nobel'\n",
      " -> 5.97: ' relativity'\n",
      " -> 8.43: ' Einstein'\n",
      " -> 9.69: ' physics'\n",
      " -> 9.88: ' theorist'\n",
      " -> 10.42: ' mathematician'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5027/3505794128.py:34: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last five Jacobian evaluations.\n",
      "  mu = fsolve(system, x0, args=(E.numpy(), amplificationAttention*vectorA.numpy()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve, least_squares\n",
    "\n",
    "# f function (to improve stability, Taylor expansion around 0)\n",
    "def f(z):\n",
    "    z = np.asarray(z) \n",
    "    tol = 0.1\n",
    "    result = np.empty_like(z, dtype=float)\n",
    "\n",
    "    small_mask = np.abs(z) < tol\n",
    "    z_small = z[small_mask]\n",
    "    result[small_mask] = (-1/3)*z_small + (2/45)*z_small**3 - (17/945)*z_small**5\n",
    "\n",
    "    # Maschera per valori \"grandi\" (|z| >= tol)\n",
    "    big_mask = ~small_mask\n",
    "    z_big = z[big_mask]\n",
    "    result[big_mask] = - 1 / np.tanh(z_big) + 1 / z_big\n",
    "\n",
    "    return result\n",
    "\n",
    "def system(x, E, A):\n",
    "    E = E[:-1, :]\n",
    "    z = np.dot(E.T, x)     \n",
    "    fz = f(z)               \n",
    "    fz = fz.reshape(-1, 1)     \n",
    "    v = E @ fz\n",
    "    v = v.reshape(-1)      \n",
    "    return v - A       \n",
    "\n",
    "\n",
    "x0 = np.random.random(n-1)      # incredibly sensible to the initial condition\n",
    "x0 = x0 / np.linalg.norm(x0)       # normalizing the initial condition\n",
    "amplificationAttention = 1000      # PERCHÉ AMPLIFICARE AIUTA A STABILIZZARE? DA CAPIRE BENE CHE MATRICI METTERE...\n",
    "mu = fsolve(system, x0, args=(E.numpy(), amplificationAttention*vectorA.numpy()))\n",
    "\n",
    "# given mu, we can compute the expected value according to the maxent model\n",
    "x_expected = f((E[:-1, :]).T @ mu)\n",
    "\n",
    "\n",
    "print(' '.join(prompt.split()[:-1]))\n",
    "prob, idxs, tokens = get_tokens_prob(torch.tensor(x_expected, dtype = torch.float), 10)\n",
    "for p, _, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> {-torch.log(p):.2f}: '{token}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
