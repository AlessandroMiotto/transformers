{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5bfe862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT AND CONFIGURATION\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True, output_attentions=True)\n",
    "embeddings = model.wte.weight.data\n",
    "\n",
    "def get_tokens_prob(x, k=5):\n",
    "    # Compute the similarity between the token and all the tokens in the vocabulary, then applies softmax\n",
    "    prob = torch.softmax(torch.matmul(x,embeddings.T), dim=-1)\n",
    "    # Get the top k tokens\n",
    "    top = torch.topk(prob, k=k)\n",
    "    idxs = top.indices\n",
    "    tokens = [tokenizer.decode(idx) for idx in idxs]\n",
    "    return top.values, idxs, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537573a0",
   "metadata": {},
   "source": [
    "## Applying MaxEnt to explore the inner working of GPT2 \n",
    "\n",
    "Let $\\vec e_1, \\vec e_2, \\dots, \\vec e_{n+1}$ be the tokens in the input prompt. We'd like to determine:\n",
    "$$\n",
    "p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)\n",
    "$$\n",
    "which is essentially what the transformer computes after $12$ repetitions of the ATTN+FFNN mechanism. In principle, since the action of the transformer (_the dynamical rule_) is known, one could potentially figure out exactly the resulting pdf in the vocabulary space. This is however analytically infeasible, since the transformer is made of several non linear layers (FFNN) and the dynamics is allegedely extremely complicated. Thus, analoguosly to what we do in stat mech, we hope our system is ergodic and try to build a statistical description on top of the whole embedding space.\n",
    "\n",
    " In particular, we resort to the __MaxEnt principle__ to compute (analytically or not) the final conditioned pdf. Note that we do have the \"correct\" pdf $p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)$. We just want to see whether, upon applying MaxEnt with the right constraints, we can obtain something similar to the distribution computed by the transformer\n",
    "\n",
    " ### Bolztmann-like approach\n",
    " The entropy functional on reads:\n",
    "\n",
    "$$\n",
    "S[p] = -k \\int d^D x \\> p(\\vec x) \\ln p(\\vec x) \n",
    "$$\n",
    "the first constraint is, obviously, the normalization $ \\int d^D x \\> p(\\vec x) = 1 $\n",
    "\n",
    "In this first paragraph, we apply the following constraint:\n",
    "$$\n",
    "\\langle \\vec x \\rangle = \\vec{e}_{n+1}\n",
    "$$\n",
    "because we do have the \"right\" next token (again, we don't want to perform prediction here with the maxEnt. We just wish to see if the resulting pdf has some similarities with the one suggested by the maximization of $S$).\n",
    "\n",
    "This leads to an analytic form for $p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n)$:\n",
    "$$\n",
    "p(\\vec x | \\vec e_1, \\vec e_2, \\dots, \\vec e_n) = p(\\vec x) = \\frac{1}{Z} \\exp(\\vec \\mu \\cdot \\vec x)\n",
    "$$\n",
    "with $Z =\\prod_{i=1}^{D} \\frac{2 sinh(\\mu_i)}{\\mu_i}$ and the vector $\\mu$ (the lagrange multiplier) is such that:\n",
    "$$\n",
    "\\vec e_{n+1} = -\\frac{1}{\\vec\\mu} + \\frac{1}{\\tanh(\\vec\\mu)} \\Longleftrightarrow \\vec\\mu = f(\\vec e_{n+1})\n",
    "$$\n",
    "\n",
    "Practically speaking, we have built a pdf Boltzmann-like where the expected value has to be the correct next token and the mode (the vector in the embedding space that maximizes the probability) is precisely $\\vec \\mu$. Let's see what $\\vec \\mu$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec612f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a multipurpose function that can be used to extract the probability density function of the next token and will be useful later.\n",
    "def GPT2extractPDFLastToken(prompt, lengthPrediction=5, returnTensors = False):\n",
    "    \"\"\"\n",
    "    This function takes a prompt masking the last word and returns the probability density function as computed by GPT 2 of what was the masked token.\n",
    "    Additionally, it returns the attention vector and the embedding matrix of the masked sequence of tokens, if needed.\n",
    "    :param prompt: The input prompt\n",
    "    :return: The probability density function of the next token\n",
    "    \"\"\"\n",
    "    prompt = ' '.join(prompt.split()[:-1])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    n = len(inputs[\"input_ids\"][0]) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        # run the transfomer inference\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        # get the hidden states (i.e. matrices of the last layer)\n",
    "    layers = hidden_states[1:]\n",
    "    # extract the attentions matrices\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # the next token is the last one in the last layer\n",
    "    nextToken = (layers[-1][0])[-1]\n",
    "    # project it into the vocabulary space and build a pdf on top of it using softmax\n",
    "    prob, idxs, tokens = get_tokens_prob(nextToken, lengthPrediction)\n",
    "\n",
    "    if (returnTensors):\n",
    "        chosenLayer = 0\n",
    "        lastStepAttentionMatrices = attentions[chosenLayer][0]\n",
    "        vectorA = torch.tensor( np.zeros(n-1) )              # Not the last token (itself w/ itself)\n",
    "        #iterate on the heads\n",
    "        for head in range(12):\n",
    "            attentionHead = lastStepAttentionMatrices[head]\n",
    "            column = attentionHead[-1,:]   # last row\n",
    "            column = column[:-1]           # remove the last element (autocorrelation)\n",
    "            vectorA = (vectorA + column)\n",
    "        # Normalization step\n",
    "        vectorA = vectorA / np.linalg.norm(vectorA) \n",
    "        T = torch.nn.functional.one_hot(inputs.input_ids[0], num_classes=embeddings.shape[0]).float()\n",
    "        E = torch.matmul(T,embeddings)\n",
    "        E = E / np.linalg.norm(E, axis=1, keepdims=True)     # Normalize again by row\n",
    "        E = E[:-1,:]                                         # remove the last token\n",
    "        return prob, idxs, tokens, vectorA, E\n",
    "    else:\n",
    "        return prob, idxs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2dc2238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> logp = 0.28: [' Galile'], idx = 32422\n",
      " -> logp = 2.29: ['.'], idx = 13\n",
      " -> logp = 3.12: [' and'], idx = 290\n",
      " -> logp = 3.55: [','], idx = 11\n",
      " -> logp = 5.17: ['.\"'], idx = 526\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm an Italian physicist who lived in Padua. I am known for my work in the field of optics and astronomy. \" \\\n",
    "        \"I was born in 1564 and died in 1642. My name is Galileo Galilei\"\n",
    "prob, idxs, tokens = GPT2extractPDFLastToken(prompt)\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> logp = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4f3cb2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully computed mu vector.\n",
      "It represents the direction of maximum probability; projecting mu over the vocabolary we get: \n",
      "\n",
      " -> logp = -0.00: [' Galile'], idx = 32422\n",
      " -> logp = 38.92: [' Canaver'], idx = 46858\n",
      " -> logp = 40.84: ['Palest'], idx = 32570\n",
      " -> logp = 40.95: [' Galileo'], idx = 45860\n",
      " -> logp = 41.26: [' Presbyter'], idx = 40507\n",
      " -> logp = 41.34: [' Palest'], idx = 5480\n",
      " -> logp = 41.82: [' Jude'], idx = 30044\n",
      " -> logp = 42.00: [' horizont'], idx = 13736\n",
      "\n",
      "As expected, it points practically with certainty to the right expected token\n"
     ]
    }
   ],
   "source": [
    "#This is the function we want to solve\n",
    "def f(x,k):\n",
    "    return 1 / np.tanh(x) - 1 / x - k\n",
    "\n",
    "# We need to constraint the mean value of the distribution to be equal to the expected value of the token\n",
    "tokenExpected = embeddings[idxs[0]]      # extracted from above\n",
    "\n",
    "# Solve it component by component\n",
    "mu = torch.tensor(np.zeros(embeddings.shape[1]), dtype=torch.float32)\n",
    "for i,d in enumerate(tokenExpected):\n",
    "    xi = d.item()\n",
    "    mu_i = fsolve(f, 1, args=(xi))\n",
    "    mu[i] = (mu_i.item())\n",
    "\n",
    "print(f\"Succesfully computed mu vector.\")\n",
    "print(\"It represents the direction of maximum probability; projecting mu over the vocabolary we get: \\n\")\n",
    "# mu should be the direction of maximum probability (mode). At what does it point to?\n",
    "prob, idxs, tokens = get_tokens_prob(mu, 8)\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> logp = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")\n",
    "\n",
    "print(\"\\nAs expected, it points practically with certainty to the right expected token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5da8c",
   "metadata": {},
   "source": [
    "This is a boring result. We did get the right token but this was, of course, by no surprise since we cooked that information into the pdf. In fact, the other tokens suggested by the Boltzmann model were probably related to the word \"Galilea\" (Palest, Jude, Canaver). Since we didn't used the previous token, we lost all the context contained by the whole sequence of the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed2a02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "DA AGGIUNGERE: LOG-LIKELI ESPONENZIALE COL PRODOTTO SCALARE, DA AGGIUNGERE? GIUSTIFICAZIONE A POSTERIORI\n",
    "\n",
    "\n",
    "### Using the attention matrix\n",
    "\n",
    "The previous approach was clearly extremely simple and trivial, lacking precious information that we have on the resulting pdf (in fact, we didn't use the previous tokens $\\{e_i\\}$). Let us think of a new constraint. Since the dot product in the embedding space (upon normalization) is somehow related to the similarities between tokens, we may wish to constrain:\n",
    "$$ \n",
    "\\langle \\vec x \\cdot \\vec e_i \\rangle = something, \\>\\> \\forall \\vec e_i\n",
    "$$\n",
    "Since the transfomer provides us with attention matrices, why don't use those quantities to constrain our entropy. In particular, if $\\hat A$ is the $n+1 \\times n+1$ attention matrix at the end of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1345878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f function (to improve stability, Taylor expansion around 0)\n",
    "def f(z, tol = 0.1):\n",
    "    z = np.asarray(z) \n",
    "    result = np.empty_like(z, dtype=float)\n",
    "    # when z is small, we use a Taylor expansion (to avoid numerical instability)\n",
    "    z_small = z[np.abs(z) < tol]\n",
    "    result[np.abs(z) < tol] = (-1/3)*z_small + (2/45)*z_small**3 - (17/945)*z_small**5\n",
    "    # when z is big, we use the original function\n",
    "    z_big = z[np.abs(z) > tol]\n",
    "    result[np.abs(z) > tol] = - 1 / np.tanh(z_big) + 1 / z_big\n",
    "    return result\n",
    "\n",
    "# This is the system we want to solve\n",
    "def system(x, E, A):\n",
    "    z = np.dot(E.T, x)     \n",
    "    fz = f(z)        \n",
    "    fz = fz.reshape(-1, 1)     \n",
    "    v = E @ fz\n",
    "    v = v.reshape(-1)      \n",
    "    return v - A      \n",
    "\n",
    "def solveMu(E, vectorA, amplificationAttention=1):\n",
    "    x0 = np.random.random(len(vectorA))      # incredibly sensible to the initial condition\n",
    "    x0 = x0 / np.linalg.norm(x0)             # normalizing the initial condition\n",
    "    mu = fsolve(system, x0, args=(E.numpy(), amplificationAttention*vectorA.numpy()))\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a380e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the last token (\"Einstein\"), GPT2 would have predicted:\n",
      " -> -logP = 0.58: [' Einstein'], idx = 24572\n",
      " -> -logP = 4.39: [' B'], idx = 347\n",
      " -> -logP = 4.67: [' von'], idx = 18042\n",
      " -> -logP = 4.79: [' Hof'], idx = 37745\n",
      " -> -logP = 4.83: ['.'], idx = 13\n",
      "\n",
      "A MaxEnt approach yields:\n",
      "For the mean value (MAXENT):\n",
      " -> 10.19: ' physicist'\n",
      " -> 10.29: ' mathemat'\n",
      " -> 10.32: ' physicists'\n",
      " -> 10.34: ' Nobel'\n",
      " -> 10.36: ' relativity'\n",
      " -> 10.40: ' chemist'\n",
      " -> 10.40: ' mathematician'\n",
      " -> 10.41: ' 1955'\n",
      " -> 10.42: ' Einstein'\n",
      " -> 10.42: ' Physics'\n",
      "For the mode value (MAXENT):\n",
      " -> 8.93: ' physicist'\n",
      " -> 9.24: ' mathemat'\n",
      " -> 9.34: ' physicists'\n",
      " -> 9.39: ' Nobel'\n",
      " -> 9.45: ' relativity'\n",
      " -> 9.56: ' chemist'\n",
      " -> 9.57: ' mathematician'\n",
      " -> 9.60: ' 1955'\n",
      "\n",
      "\n",
      "Error on A:  tensor([-0.3714,  0.7095,  0.1139, -0.0253,  0.7658,  0.1468,  0.1167, -0.1898,\n",
      "         0.1799,  0.0827,  0.0938, -0.1063,  0.4455,  0.7829,  0.1868, -0.1405,\n",
      "         0.3817, -0.0531,  1.1250,  1.2763,  1.6499, -0.1436,  0.0263,  0.0580,\n",
      "        -0.0876,  0.5544, -0.0391,  0.5062,  0.2997,  0.4588, -0.0611,  0.0450,\n",
      "         0.0316, -0.1775,  0.0977, -0.0432, -0.0418,  0.4062,  0.4902,  0.7323,\n",
      "        -0.0696, -0.0023, -0.0930, -0.0807, -0.2092, -0.1129, -0.3449, -0.1453,\n",
      "        -0.4714, -0.2810,  0.0353, -0.5561], dtype=torch.float64) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6486/785929897.py:25: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  mu = fsolve(system, x0, args=(E.numpy(), amplificationAttention*vectorA.numpy()))\n",
      "/tmp/ipykernel_6486/3445847984.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prob, idxs, tokens = get_tokens_prob(torch.tensor(x_mode, dtype = torch.float), 8)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The american flag is white, red and blue. The flag has 13 stripes and 50 stars\"\n",
    "prompt = \"Mount Everest is the tallest mountain on Earth, located in the Himalayas on the border between Nepal and China\"\n",
    "prompt = \"DNA, or deoxyribonucleic acid, is the molecule that carries genetic instructions in all living organisms. It is composed of four nucleotide bases: adenine, thymine, cytosine, and guanine. These bases form sequences that determine traits and biological functions.\"\n",
    "#prompt = \"The year is 2194, and humanity no longer lives on Earth. After the Collapse, we fled to floating arcologies orbiting the gas giants. Every child knows the stories of our homeworld, but none of us have seen a tree, felt rain, or walked on soil. Until today.\"\n",
    "prompt = \"I'm an Italian physicist who lived in Padua. I am known for my work in the field of optics and astronomy. \" \\\n",
    "        \"I was born in 1564 and died in 1642. My name is Galileo\"\n",
    "prompt = \"The american flag is white, red and blue\"\n",
    "prompt = \"I'm a German-born theoretical physicist who revolutionized modern physics with my theory of relativity. \" \\\n",
    "\"I was awarded the Nobel Prize in Physics in 1921 for my explanation of the photoelectric effect.\" \\\n",
    "\" I was born in 1879 and died in 1955. My name is Albert Einstein\"\n",
    "\n",
    "prob, idxs, tokens = GPT2extractPDFLastToken(prompt)\n",
    "print(\"Masking the last token (\\\"\" , ''.join(prompt.split()[-1]), \"\\\"), GPT2 would have predicted:\", sep = \"\")\n",
    "for p, idx, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> -logP = {-torch.log(p):.2f}: ['{token}'], idx = {idx}\")\n",
    "\n",
    "# Extract the needed matrices from the GPT2 model (embedding matrix and attention vector)\n",
    "_,_,_, vectorA, E = GPT2extractPDFLastToken(prompt, returnTensors=True)\n",
    "# Let's solve for mu, now that we have the attention vector and the embedding matrix\n",
    "amplificationAttention = 1\n",
    "mu = solveMu(E, vectorA, amplificationAttention)\n",
    "# given mu, we can compute the mean and the mode according to the maxent model\n",
    "x_avg, x_mode = f(E.T @ mu), -E.T @ mu\n",
    "\n",
    "print(\"\\nA MaxEnt approach yields:\")\n",
    "print(\"For the mean value (MAXENT):\")\n",
    "prob, idxs, tokens = get_tokens_prob(torch.tensor(x_avg, dtype = torch.float), 10)\n",
    "for p, _, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> {-torch.log(p):.2f}: '{token}'\")\n",
    "\n",
    "print(\"For the mode value (MAXENT):\")\n",
    "prob, idxs, tokens = get_tokens_prob(torch.tensor(x_mode, dtype = torch.float), 8)\n",
    "for p, _, token in zip(prob, idxs, tokens):\n",
    "    print(f\" -> {-torch.log(p):.2f}: '{token}'\")\n",
    "\n",
    "expectedA = E @ x_avg\n",
    "print( \"\\n\\nError on A: \", (expectedA/amplificationAttention - vectorA)/vectorA, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716ed69",
   "metadata": {},
   "source": [
    "The expected value is not that bad (comments)\n",
    "\n",
    "Nota bene una cosa: qua stiamo ponendo solo $n$ condizioni, ma la distribuzione è su di uno spazio $ d > n $ dimensionale. Dunque stiamo dicendo che il valore medio \\langle x \\rangle non è completamente vincolato (come nel caso precedente), ne stiamo vincolando solo una parte ($n$ componenti, linearmente). Il resto fa maxEnt. Il fatto che qua il valore medio venga decente, almento secondo me, non è scontato (soprattutto se $n$ è molto più piccolo di $d$).\n",
    "\n",
    "Sarebbe carino capire come migliora il modello aggiungendo vincoli, cioè aumentando la dimensione del prompt.\n",
    "\n",
    "----\n",
    "Let's explore a bit the linear approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0d88ab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det(G) is:  -0.0\n",
      "Given prompt: \" I'm a German-born theoretical physicist who revolutionized modern physics with my theory of relativity. I was awarded the Nobel Prize in Physics in 1921 for my explanation of the photoelectric effect. I was born in 1879 and died in 1955. My name is Albert \"\n",
      "Correct next token:  Einstein\n",
      "\n",
      "Expected value (MAXENT):\n",
      " -> 5.77: ' mathemat'\n",
      " -> 7.50: ' challeng'\n",
      " -> 7.54: ' rul'\n",
      " -> 7.65: ' nodd'\n",
      " -> 7.65: ' horizont'\n",
      " -> 7.68: ' arrang'\n",
      " -> 7.68: ' advoc'\n",
      " -> 7.71: ' distingu'\n",
      " -> 7.71: ' destro'\n",
      " -> 7.73: ' defic'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6486/2014146491.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prob, idxs, tokens = get_tokens_prob(torch.tensor(x_avg, dtype = torch.float), 10)\n"
     ]
    }
   ],
   "source": [
    "# Gram matrix\n",
    "G = E @ E.T\n",
    "print(\"The det(G) is: \", np.linalg.det(G))\n",
    "\n",
    "if (np.linalg.matrix_rank(G) == G.shape[0]):\n",
    "    mu_linear = np.linalg.solve(G, -3.*vectorA)\n",
    "    x_avg, x_mode = E.T @ vectorA, np.linalg.inv(G) @ mu_linear\n",
    "\n",
    "    print(\"Given prompt: \\\"\", ' '.join(prompt.split()[:-1]), \"\\\"\")\n",
    "    print(\"Correct next token: \", ''.join(prompt.split()[-1]))\n",
    "    print(\"\\nExpected value (MAXENT):\")\n",
    "    prob, idxs, tokens = get_tokens_prob(torch.tensor(x_avg, dtype = torch.float), 10)\n",
    "    for p, _, token in zip(prob, idxs, tokens):\n",
    "        print(f\" -> {-torch.log(p):.2f}: '{token}'\")\n",
    "\n",
    "    print(\"Mode value (MAXENT):\")\n",
    "    prob, idxs, tokens = get_tokens_prob(torch.tensor(x_mode, dtype = torch.float), 8)\n",
    "    for p, _, token in zip(prob, idxs, tokens):\n",
    "        print(f\" -> {-torch.log(p):.2f}: '{token}'\")\n",
    "\n",
    "    expectedA = E @ x_avg\n",
    "    #print( \"\\n\\nError on A: \", (expectedA/amplificationAttention - vectorA)/vectorA, \"\\n\\n\")\n",
    "else:\n",
    "    x_avg = E.T @ vectorA.float()\n",
    "    print(\"Given prompt: \\\"\", ' '.join(prompt.split()[:-1]), \"\\\"\")\n",
    "    print(\"Correct next token: \", ''.join(prompt.split()[-1]))\n",
    "    print(\"\\nExpected value (MAXENT):\")\n",
    "    prob, idxs, tokens = get_tokens_prob(torch.tensor(x_avg, dtype = torch.float), 10)\n",
    "    for p, _, token in zip(prob, idxs, tokens):\n",
    "        print(f\" -> {-torch.log(p):.2f}: '{token}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLabComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
